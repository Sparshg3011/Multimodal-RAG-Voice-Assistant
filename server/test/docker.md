Hereâ€™s a **step-by-step guide** to set up and run **LLMs locally using Docker Model Runner** using (Docker Desktop v4.40+):

---

## ğŸ› ï¸ Prerequisites

* âœ… **Docker Desktop v4.40 or higher**
* âœ… Internet connection (to pull models)
* âœ… Basic CLI knowledge

---

## ğŸš€ Step-by-Step Setup Guide

### ğŸ”¹ 1. **Install or Update Docker Desktop**

If you havenâ€™t already:

ğŸ”— Download: [https://www.docker.com/products/docker-desktop](https://www.docker.com/products/docker-desktop)

Make sure it's version **4.40 or later** to support Model Runner.

---

### ğŸ”¹ 2. **Enable Model Runner (if not already enabled)**

Model Runner is usually **enabled by default**.

But to be sure, run this command in your terminal:

```bash
docker desktop enable model-runner
```

âœ… To expose the API via TCP (to use in code):

```bash
docker desktop enable model-runner --tcp 12434
```

---

### ğŸ”¹ 3. **Pull a Local LLM Model**

Example using a lightweight model:

```bash
docker model pull ai/smollm2:360M-Q4_K_M
```

ğŸ‘‰ You can explore more models in the `ai/` namespace on Docker Hub:
ğŸ”— [https://hub.docker.com/search?q=ai%2F\&type=model](https://hub.docker.com/search?q=ai%2F&type=model)

---

### ğŸ”¹ 4. **Run the Model from CLI**

Test it by chatting with the model:

```bash
docker model run ai/smollm2:360M-Q4_K_M "Give me a fact about whales."
```

ğŸ’¬ The model will respond with a fact using native llama.cpp-based inference!

---

### ğŸ”¹ 5. **Use from Your Code (Optional)**

You can use the Model Runner as a drop-in **OpenAI-compatible API**!

ğŸ“ Host endpoint:
`http://localhost:12434/engines/v1`

Example using **langchain-openai (Python)**:

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    base_url="http://localhost:12434/engines/v1",
    api_key="docker",
    model="ai/llama3.2"
)

response = llm.invoke("Tell me about india??")
print(response.content)
```

---

## ğŸ“Œ Notes

* ğŸ§  Models are stored locally and cached in memory (5-min timeout).
* ğŸ“¦ You **donâ€™t need containers** â€” it runs natively on host.
* ğŸ” You can switch models by changing the tag.

---

## âœ… Youâ€™re Ready!

Youâ€™ve now set up a fully local, OpenAI-compatible LLM experience on your Mac using Docker. Run it offline, embed it in your dev workflow, or build chat apps â€” without external APIs!

